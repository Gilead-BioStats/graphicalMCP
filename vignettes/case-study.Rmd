---
title: "case-study"
output:
  rmarkdown::html_vignette:
    code_folding: "hide"
vignette: >
  %\VignetteIndexEntry{case-study}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(graphicalMCP)
library(printr)
```

```{r}
knit_print.initial_graph = function(x, ...) {
  res = as.data.frame(cbind(x$hypotheses, x$transitions))
  gt::gt(res, rownames_to_stub = TRUE)
}

registerS3method(
  "knit_print", "initial_graph", knit_print.initial_graph,
  envir = asNamespace("knitr")
)
```

This vignette serves to demonstrate testing and power calculations for a moderately complex graph example. The graph we'll create has a Bonferroni Holm structure between two groups, each of which have a parallel gate-keeping structure with one primary hypothesis and two secondary hypotheses.

### Initial graph

```{r initial-graph}
eps <- .0001

hypotheses <- c(rep(c(1 / 2, 0, 0), 2))
transitions <- rbind(
  c(0, .5, .5, 0, 0, 0),
  c(0, 0, 1, 0, 0, 0),
  c(0, 1 - eps, 0, eps, 0, 0),
  c(0, 0, 0, 0, .5, .5),
  c(0, 0, 0, 0, 0, 1),
  c(eps, 0, 0, 0, 1 - eps, 0)
)

g_complex <- create_graph(hypotheses, transitions)

print(g_complex)

knit_print.initial_graph(g_complex)
```

```{r printy, echo=FALSE}
knitr::knit_print(g_complex$hypotheses)

knitr::knit_print(g_complex$transitions, options = NULL)
```

```{r rendery, echo=FALSE}
knitr::kable(g_complex$hypotheses)

knitr::kable(g_complex$transitions)
```

```{r study-results}
cohort_size <- 200

# simulate performance of each endpoint against placebo
study_results <- list(
  H1 = rnorm(cohort_size, 4),
  H2 = rnorm(cohort_size, .1, 2),
  H3 = rnorm(cohort_size, 0, .1),
  H4 = rnorm(cohort_size, 3, 3),
  H5 = rnorm(cohort_size, 1),
  H6 = rnorm(cohort_size, 4, .5)
)

t_tests <- lapply(study_results, t.test, alternative = "greater")
```

In this example, 

The two primary hypotheses will be tested as a parametric group. Correlation between the primary hypotheses' test statistics is 0.5, as is the case when...[find paper that talks about this - 2010?]. P-values are chosen to illustrate a blend of accept/reject decisions. Testing and power will be calculated at the global level 0.025.

```{r test}
p_vals <- c(.01, .001, .02, .02, .001, .002)

t_corr <- matrix(.5, nrow = 6, ncol = 6)
diag(t_corr) <- 1

test_graph(
  g_complex,
  p_vals,
  alpha = .025,
  groups = list(c(1, 4), 2:3, 5:6),
  test_types = c("p", "s", "s"),
  corr = t_corr
)
```

It's not always obvious from a graph how easy or difficult it will be to reject each hypothesis. One way to understand this better is to run a power simulation. The essence of a power simulation is to generate many different p-values using some chosen distribution, then test the graph against each set of p-values to see how it performs.

The p-values are sampled from a multivariate normal distribution using a one-sided test. The means of the MVN should be set as the marginal power of each hypothesis, and `sim_corr` should be the correlation between hypotheses that is induced by the study design.

```{r power}
calculate_power(g_complex, sim_n = 1e5)
```


