---
title: "Testing basics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Testing basics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(graphicalMCP)
library(gMCP)
```

Start testing with the example graph from the README, a parallel gate-keeping procedure graph.

```{r create-graph-1}
# A graphical multiple comparison procedure with two primary hypotheses (H1
# and H2) and two secondary hypotheses (H3 and H4)
# See Figure 1 in Bretz, F., Posch, M., Glimm, E., Klinglmueller, F., Maurer,
# W., & Rohmeyer, K. (2011). Graphical approaches for multiple comparison
# procedures using weighted Bonferroni, Simes, or parametric tests. Biometrical
# Journal, 53(6), 894-913.
hypotheses <- c(0.5, 0.5, 0, 0)
transitions <- rbind(
  c(0, 0, 1, 0),
  c(0, 0, 0, 1),
  c(0, 1, 0, 0),
  c(1, 0, 0, 0)
)
names <- c("A1", "A2", "B1", "B2")
par_gate <- create_graph(hypotheses, transitions, names)

pvals <- c(.024, .01, .026, .027)

par_gate
```

This graph can be tested most simply with the default weighted Bonferroni test. When testing at the global alpha level 0.05, we can reject hypotheses A1, A2, and B1, but not B2.

```{r bonferroni-mix-1}
test_graph(par_gate, p = pvals, alpha = .05, critical = TRUE)
```

The results of the weighted Simes test are equivalent to weighted Bonferroni in some situations. The power of the Simes test becomes apparent when multiple p-values fall below the global alpha level, but above their local alpha in some intersection(s). In the following case, B1 & B2 are rejected in the Bonferroni testing procedure for intersection `B1 ∩ B2` because the p-value is greater than `α * w` for each hypothesis in that case. However, the Simes test rejects `B1 ∩ B2` because the weight from B1 is added to the weight for B2.

```{r simes-all-1}
test_graph(par_gate, p = pvals, alpha = .05, tests = "s")
```

If a correlation matrix for the test statistics is partially or fully known, a parametric test can be used for any subsets whose correlation matrix is fully known. Here B1 & B2 get a `c` value calculated that boosts their testing threshold slightly higher. 

```{r parametric-1}
corr1 <- matrix(nrow = 4, ncol = 4)
corr1[3:4, 3:4] <- .5
diag(corr1) <- 1

test_graph(par_gate,
  p = pvals,
  alpha = .05,
  groups = list(1, 2, 3:4),
  tests = c("p", "p", "p"),
  corr = corr1
)
```

 ~~The parametric test reduces to Bonferroni when there is no correlation between any test statistics.~~ (This doesn't look like it's true, actually)

```{r parametric-2}
corr2 <- diag(4)

test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = NULL,
  tests = "p"
)
```

The null case of this is when a parametric group is size 1 (Each correlation matrix is a 1x1 with value 1)

```{r parametric-3}
corr3 <- matrix(nrow = 4, ncol = 4)
diag(corr3) <- 1

test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = corr3, # Correlation matrix doesn't matter when each group is size 1
  groups = list(1, 2, 3, 4),
  tests = rep("p", 4)
)
```

Using different test types on different parts of a graph is supported.

```{r mixed-1}
test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = corr1,
  groups = list(1, 2, 3:4),
  tests = c("s", "b", "p")
)
```

There are two different testing methods - one which tests each hypothesis with the `p <= (c *) w * α` method, and another which calculates adjusted p-values. The adjusted p-values method is much more efficient, so it is the standard method. Additional details about the adjusted p-values calculation can be seen by setting `verbose = TRUE`.

```{r verbose}
test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = corr1,
  groups = list(1, 2, 3:4),
  tests = c("s", "b", "p"),
  verbose = TRUE
)
```

The critical value method tests every hypothesis in the closure. Setting `critical = TRUE` displays the values used in each of these tests. This can provide more detailed information about what caused a hypothesis to fail than the adjusted p-values. However, it comes at a significant cost in computation time.

```{r critical}
test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = corr1,
  groups = list(1, 2, 3:4),
  tests = c("s", "b", "p"),
  verbose = TRUE,
  critical = TRUE
)
```

```{r benchmark}
set.seed(3123)
# Randomly generate a graph
m <- 10
w <- sample(1:m, replace = T)
w <- w / sum(w)
g <- replicate(m, sample(1:m, replace = T), simplify = T)
diag(g) <- 0
g <- g / rowSums(g)
graph <- new("graphMCP", m = g, weights = w)
graph2 <- create_graph(w, g)

p <- runif(m, .0001, .05)
corr <- diag(m)

# Does timing, and also checks results are equal
# Bonferroni
bench::mark(
  bonferroni_sequential(graph2, p)$rejected,
  test_graph(graph2, p)$outputs$rejected,
  test_graph(graph2, p, verbose = TRUE)$outputs$rejected,
  test_graph(graph2, p, critical = TRUE)$outputs$rejected,
  gMCP(graph, p)@rejected,
  check = FALSE
)

# Simes
bench::mark(
  test_graph(graph2, p, tests = "s")$outputs$rejected,
  test_graph(graph2, p, tests = "s", verbose = TRUE)$outputs$rejected,
  test_graph(graph2, p, tests = "s", critical = TRUE)$outputs$rejected,
  gMCP(graph, p, test = "Simes")@rejected
)

# Parametric
bench::mark(
  test_graph(graph2, p, tests = "p", corr = corr)$outputs$rejected,
  test_graph(graph2, p, tests = "p", corr = corr, verbose = TRUE)$outputs$rejected,
  test_graph(graph2, p, tests = "p", corr = corr, critical = TRUE)$outputs$rejected,
  gMCP(graph, p, test = "parametric", correlation = corr)@rejected
)
```






