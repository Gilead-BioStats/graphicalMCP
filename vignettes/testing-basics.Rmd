---
title: "Testing basics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Testing basics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(graphicalMCP)
library(gMCP)
```

## Testing parameters

Start testing with the example graph from the README, a parallel gate-keeping procedure graph.

```{r create-graph-1}
# A graphical multiple comparison procedure with two primary hypotheses (H1
# and H2) and two secondary hypotheses (H3 and H4)
# See Figure 1 in Bretz, F., Posch, M., Glimm, E., Klinglmueller, F., Maurer,
# W., & Rohmeyer, K. (2011). Graphical approaches for multiple comparison
# procedures using weighted Bonferroni, Simes, or parametric tests. Biometrical
# Journal, 53(6), 894-913.
hypotheses <- c(0.5, 0.5, 0, 0)
transitions <- rbind(
  c(0, 0, 1, 0),
  c(0, 0, 0, 1),
  c(0, 1, 0, 0),
  c(1, 0, 0, 0)
)
names <- c("A1", "A2", "B1", "B2")
par_gate <- create_graph(hypotheses, transitions, names)

pvals <- c(.024, .01, .026, .027)

par_gate
```

This graph can be tested most simply with the default weighted Bonferroni test. When testing at the global alpha level 0.05, we can reject hypotheses A1, A2, and B1, but not B2.

```{r bonferroni-mix-1}
test_graph(par_gate, p = pvals, alpha = .05)
```

The results of the weighted Simes test are equivalent to weighted Bonferroni in some situations. The power of the Simes test becomes apparent when multiple p-values fall below the global alpha level, but above their local alpha in some intersection(s). In the following case, B1 & B2 are rejected in the Bonferroni testing procedure for intersection `B1 ∩ B2` because the p-value is greater than `α * w` for each hypothesis in that case. However, the Simes test rejects `B1 ∩ B2` because the weight from B1 is added to the weight for B2.

```{r simes-all-1}
test_graph(par_gate, p = pvals, alpha = .05, test_types = "s")
```

If a correlation matrix for the test statistics is partially or fully known, a parametric test can be used for any subsets whose correlation matrix is fully known. Here B1 & B2 get a `c` value calculated that boosts their testing threshold slightly higher. 

```{r parametric-1}
corr1 <- matrix(nrow = 4, ncol = 4)
corr1[3:4, 3:4] <- .5
diag(corr1) <- 1

test_graph(par_gate,
  p = pvals,
  alpha = .05,
  groups = list(1, 2, 3:4),
  test_types = c("b", "b", "p"),
  corr = corr1
)
```

 ~~The parametric test reduces to Bonferroni when there is no correlation between any test statistics.~~ (This doesn't look like it's true, actually)

```{r parametric-2}
corr2 <- diag(4)

test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = diag(4),
  test_types = "p"
)
```

The null case of this is when a parametric group is size 1 (Each correlation matrix is a 1x1 with value 1)

```{r parametric-3}
corr3 <- matrix(nrow = 4, ncol = 4)
diag(corr3) <- 1

test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = corr3, # Correlation matrix doesn't matter when each group is size 1
  groups = list(1, 2, 3, 4),
  test_types = rep("p", 4)
)
```

Using different test types on different parts of a graph is supported.

```{r mixed-1}
test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = corr1,
  groups = list(1:2, 3:4),
  test_types = c("s", "p")
)
```

There are two different testing methods - one which tests each hypothesis with the `p <= (c *) w * α` method, and another which calculates adjusted p-values. The adjusted p-values method is much more efficient, so it is the standard method. Additional details about the adjusted p-values calculation can be seen by setting `verbose = TRUE`.

```{r verbose}
test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = corr1,
  groups = list(1:2, 3:4),
  test_types = c("s", "p"),
  verbose = TRUE
)
```

The critical value method tests every hypothesis in the closure. Setting `critical = TRUE` displays the values used in each of these tests. This can provide more detailed information about what caused a hypothesis to fail than the adjusted p-values. However, it comes at a significant cost in computation time.

```{r critical}
test_graph(
  par_gate,
  p = pvals,
  alpha = .05,
  corr = corr1,
  groups = list(1:2, 3:4),
  test_types = c("s", "p"),
  verbose = TRUE,
  critical = TRUE
)
```

## Performance

`test_graph()` automatically chooses the sequentially rejective test if all tests are Bonferroni, and if the verbose and critical flags are not selected. This shortcut significantly reduces computation time. If optimal performance is needed, `bonferroni_sequential()` can be used directly. However, input validation is not present, so errors may be more cryptic. Note that the structure returned by the sequentially rejective test is slightly different than the other tests.

```{r benchmark-bonf}
set.seed(3123)
# Randomly generate a graph
m <- 5
w <- sample(1:m, replace = T)
w <- w / sum(w)
g <- replicate(m, sample(1:m, replace = T), simplify = T)
diag(g) <- 0
g <- g / rowSums(g)
graph <- new("graphMCP", m = g, weights = w)
graph2 <- create_graph(w, g)

p <- runif(m, .0001, .05)
sim_corr <- diag(m)

# Bonferroni
bench::mark(
  bonferroni_sequential(graph2, p)$outputs$rejected,
  test_graph(graph2, p)$outputs$rejected,
  test_graph(graph2, p, verbose = TRUE)$outputs$rejected,
  test_graph(graph2, p, critical = TRUE)$outputs$rejected,
  gMCP(graph, p)@rejected,
  check = FALSE
)
```

Simes testing is a bit slower than Bonferroni because the adjusted p-value calculation is more complicated. However, graphicalMCP performance is still >100x faster than gMCP.

```{r benchmark-simes}
# Simes
bench::mark(
  test_graph(graph2, p, test_types = "s")$outputs$rejected,
  test_graph(graph2, p, test_types = "s", verbose = TRUE)$outputs$rejected,
  test_graph(graph2, p, test_types = "s", critical = TRUE)$outputs$rejected,
  gMCP(graph, p, test = "Simes")@rejected
)
```

Parametric testing is the slowest test, especially when `critical = TRUE`. Parametric testing in graphicalMCP is only 8-9x faster than gMCP, because the time is dominated by the multivariate normal calculation.

```{r benchmark-para}
# Parametric
bench::mark(
  test_graph(graph2, p, test_types = "p", corr = sim_corr)$outputs$rejected,
  test_graph(graph2, p, test_types = "p", corr = sim_corr, verbose = TRUE)$outputs$rejected,
  test_graph(graph2, p, test_types = "p", corr = sim_corr, critical = TRUE)$outputs$rejected,
  gMCP(graph, p, test = "parametric", correlation = sim_corr)@rejected
)
```

## Print options

The print generic for test results includes a couple of additional options. Each section within results is indented 2 spaces by default, but this can be adjusted with `indent`. Numeric values are rounded to 6 decimals to control the amount of space used, but this can be set using the `precision` argument. This only affects the printing format, not the underlying values.

```{r print-indent}
set.seed(3123)
# Randomly generate a graph
m <- 5
w <- sample(1:m, replace = T)
w <- w / sum(w)
g <- replicate(m, sample(1:m, replace = T), simplify = T)
diag(g) <- 0
g <- g / rowSums(g)
graph <- new("graphMCP", m = g, weights = w)
graph2 <- create_graph(w, g)

p <- runif(m, .0001, .05)
sim_corr <- diag(m)

mix_test <- test_graph(
  graph2,
  p = p,
  alpha = .05,
  corr = sim_corr,
  groups = list(1, 2:3, 4:5),
  test_types = c("b", "s", "p"),
  verbose = TRUE,
  critical = TRUE
)

print(mix_test)

print(mix_test, indent = 6, precision = 10)
```
